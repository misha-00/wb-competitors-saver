# wb_competitors_app.py
# FAST –≤–µ—Ä—Å–∏—è: –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–∫–∞—á–∫–∞ —Ñ–æ—Ç–æ (aiohttp) + –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º (–ø–æ –æ–¥–Ω–æ–º—É —Å–ª–∞–π–¥—É) + Excel —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ + –∫–æ–ª–ª–∞–∂ + ZIP + –∞–≤—Ç–æ–æ—á–∏—Å—Ç–∫–∞

import asyncio
import io
import json
import math
import pathlib
import re
import shutil
import zipfile
from datetime import datetime
from io import BytesIO
from urllib.parse import urlparse, parse_qs

import pandas as pd
import requests
import streamlit as st
from PIL import Image
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ---- aiohttp (–¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å–∫–∞—á–∫–∏) ----
from aiohttp import ClientSession, TCPConnector, ClientTimeout

# ================== –ù–ê–°–¢–†–û–ô–ö–ò ==================
# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å
GLOBAL_CONN_LIMIT = 64          # –æ–±—â–∏–π –ª–∏–º–∏—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∫ CDN WB
PER_PRODUCT_WORKERS = 8         # –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å–ª–∞–π–¥–æ–≤ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
# –¢–∞–π–º–∞—É—Ç—ã
HTTP_TIMEOUT = ClientTimeout(total=20)     # aiohttp
REQ_TIMEOUT = (5, 12)                      # requests (connect, read)
RETRY_TOTAL = 2
# –ü—Ä–æ—á–µ–µ
DEFAULT_SLIDES = 10
THUMB = (360, 360)             # –ø—Ä–µ–≤—å—é –≤ –∫–æ–ª–ª–∞–∂–µ
CELL_PX = (160, 160)           # —Ä–∞–∑–º–µ—Ä –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ Excel (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)

# ================== UI ==================
st.set_page_config(page_title="WB Competitors Saver (FAST + Progress)", page_icon="‚ö°", layout="wide")
st.title("‚ö° WB –∞–Ω–∞–ª–∏–∑ –ª–∏—Å—Ç–∏–Ω–≥–∞")
st.caption(
    "–í—Å—Ç–∞–≤—å —Å—Å—ã–ª–∫–∏ WB (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ) ‚Üí –Ω–∞–∂–º–∏ **¬´–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç¬ª**.\n"
    "–°–µ—Ä–≤–∏—Å —Å–∫–∞—á–∞–µ—Ç —Ñ–æ—Ç–æ –ø–æ –∞—Ä—Ç–∏–∫—É–ª–∞–º (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ), —Å–æ–±–µ—Ä—ë—Ç **Excel —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏**, **–∫–æ–ª–ª–∞–∂** –∏ **ZIP**."
)

# ================== –£–¢–ò–õ–ò–¢–´ ==================
def ensure_dir(p: pathlib.Path):
    p.mkdir(parents=True, exist_ok=True)

def sanitize_name(name: str) -> str:
    s = (name or "").strip()
    if not s:
        return "WB_Save"
    s = re.sub(r"[^\w\- ]+", "", s, flags=re.U)
    s = re.sub(r"\s+", "_", s)
    return s or "WB_Save"

def new_unique_root(name_hint: str | None = None) -> pathlib.Path:
    base = sanitize_name(name_hint or "WB_Save")
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    root = pathlib.Path.cwd() / f"{base}_{ts}"
    root.mkdir(parents=True, exist_ok=True)
    return root

def parse_input_urls(text: str) -> list[str]:
    return [u.strip() for u in (text or "").splitlines() if u.strip()]

# ---------- requests-—Å–µ—Å—Å–∏—è (–¥–ª—è card JSON, —Å—Ç–∞–±–∏–ª—å–Ω–æ) ----------
def make_requests_session() -> requests.Session:
    s = requests.Session()
    s.headers.update({
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Accept-Encoding": "identity",
        "Connection": "keep-alive",
    })
    retry = Retry(
        total=RETRY_TOTAL, connect=RETRY_TOTAL, read=RETRY_TOTAL,
        backoff_factor=0.4, status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET", "HEAD"])
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=64, pool_maxsize=64)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    return s

# ================== WB HELPERS ==================
def extract_nm_id(url: str) -> str | None:
    try:
        u = urlparse(url)
        q = parse_qs(u.query)
        if "nm" in q and q["nm"]:
            return re.sub(r"\D", "", q["nm"][0])
        m = re.search(r"/catalog/(\d+)", u.path)
        if m:
            return m.group(1)
    except Exception:
        pass
    return None

def fetch_card_json(session: requests.Session, nm: str) -> dict | None:
    api = (f"https://card.wb.ru/cards/v2/detail"
           f"?appType=1&curr=rub&dest=-1257786&spp=0&nm={nm}")
    r = session.get(api, timeout=REQ_TIMEOUT)
    r.raise_for_status()
    data = r.json()
    prods = data.get("data", {}).get("products", [])
    return prods[0] if prods else None

def parse_basics(prod: dict) -> tuple[str | None, str | None, int]:
    if not prod:
        return None, None, 0
    title = prod.get("name")
    brand = prod.get("brand")
    pics = int(prod.get("pics") or 0)
    if pics == 0:
        photos = (prod.get("media") or {}).get("photos") or []
        pics = len(photos)
    return title, brand, pics

def candidate_image_urls(nm_id: int, idx: int) -> list[str]:
    vol = nm_id // 100000
    part = nm_id // 1000
    exts = (".webp", ".jpg")  # webp –æ–±—ã—á–Ω–æ –ª–µ–≥—á–µ
    baskets = [f"https://basket-{i:02d}.wb.ru" for i in range(1, 33)]
    baskets += [f"https://basket-{i:02d}.wbbasket.ru" for i in range(1, 33)]
    urls = []
    for host in baskets:
        base = f"{host}/vol{vol}/part{part}/{nm_id}/images/big/{idx}"
        for ext in exts:
            urls.append(base + ext)
    return urls

# ================== –°–ö–ê–ß–ö–ê (ASYNC FAST) ==================
async def download_one_image_async(session: ClientSession, urls: list[str], dest_path: pathlib.Path) -> bool:
    """–ü—Ä–æ–±—É–µ–º —Å–ø–∏—Å–æ–∫ –∑–µ—Ä–∫–∞–ª –ø–æ –æ—á–µ—Ä–µ–¥–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π."""
    if dest_path.with_suffix(".webp").exists() or dest_path.with_suffix(".jpg").exists():
        return True
    for u in urls:
        try:
            async with session.get(u) as r:
                if r.status == 200:
                    data = await r.read()
                    if data:
                        ext = ".webp" if u.endswith(".webp") else ".jpg"
                        dest = dest_path.with_suffix(ext)
                        dest.write_bytes(data)
                        return True
        except Exception:
            pass
    return False

async def download_product_fast(session: ClientSession, nm: int, pics: int, subdir: pathlib.Path) -> int:
    """–°–∫–∞—á–∫–∞ –≤—Å–µ—Ö —Å–ª–∞–π–¥–æ–≤ —Ç–æ–≤–∞—Ä–∞ —Å –ª–∏–º–∏—Ç–æ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á."""
    ensure_dir(subdir)
    sem = asyncio.Semaphore(PER_PRODUCT_WORKERS)

    async def _one(i: int):
        async with sem:
            urls = candidate_image_urls(nm, i)
            ok = await download_one_image_async(session, urls, subdir / f"{i}")
            return 1 if ok else 0

    tasks = [_one(i) for i in range(1, pics + 1)]
    results = await asyncio.gather(*tasks, return_exceptions=False)
    return sum(results)

async def run_fast_batch(items: list[tuple[int, int, pathlib.Path]]) -> dict[int, int]:
    """
    items: [(nm, pics, subdir), ...]
    return: {nm: saved_count}
    """
    connector = TCPConnector(limit=GLOBAL_CONN_LIMIT, ssl=False)
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Accept-Encoding": "identity",
        "Connection": "keep-alive",
    }
    result: dict[int, int] = {}
    async with ClientSession(connector=connector, timeout=HTTP_TIMEOUT, headers=headers) as session:
        # –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        async def _run_one(nm, pics, subdir):
            saved = await download_product_fast(session, nm, pics, subdir)
            result[nm] = saved

        await asyncio.gather(*(_run_one(nm, pics, subdir) for nm, pics, subdir in items))
    return result

# ================== –°–ö–ê–ß–ö–ê (DETAILED, SYNC) ==================
def download_product_images_detailed_sync(reqs: requests.Session, nm: int, pics: int,
                                          subdir: pathlib.Path,
                                          progress_bar, status_text) -> int:
    """–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ ‚Äî –±–µ–∑–æ–ø–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ª–∞–π–¥—É."""
    ensure_dir(subdir)
    saved = 0
    progress_bar.progress(0.0)
    for i in range(1, pics + 1):
        urls = candidate_image_urls(nm, i)
        ok = False
        for u in urls:
            try:
                r = reqs.get(u, timeout=REQ_TIMEOUT, stream=False)
                if r.status_code == 200 and int(r.headers.get("Content-Length", "1")) > 0:
                    ext = ".webp" if u.endswith(".webp") else ".jpg"
                    (subdir / f"{i}{ext}").write_bytes(r.content)
                    ok = True
                    break
            except Exception:
                pass
        saved += 1 if ok else 0
        status_text.write(f"–°–ª–∞–π–¥ {i}/{pics} ‚Äî {'OK' if ok else '–ø—Ä–æ–ø—É—Å–∫'}")
        progress_bar.progress(i / pics)
    return saved

# ================== –ü–û–°–¢-–û–ë–†–ê–ë–û–¢–ö–ê ==================
def detect_max_slides(root: pathlib.Path) -> int:
    max_slides = 0
    for sub in root.iterdir():
        if not sub.is_dir():
            continue
        imgs = list(sub.glob("*.jpg")) + list(sub.glob("*.webp"))
        if not imgs:
            continue
        local_max = 0
        for p in imgs:
            try:
                local_max = max(local_max, int(p.stem))
            except Exception:
                pass
        if local_max == 0:
            local_max = len(imgs)
        max_slides = max(max_slides, local_max)
    return max_slides or 1

def _image_to_png_bytes(path: pathlib.Path, max_w: int, max_h: int) -> BytesIO | None:
    try:
        im = Image.open(path).convert("RGB")
        im.thumbnail((max_w, max_h))
        bio = BytesIO()
        im.save(bio, format="PNG", optimize=True)
        bio.seek(0)
        return bio
    except Exception:
        return None

def save_excel_with_images(root: pathlib.Path,
                           summary_rows: list[dict],
                           limit_slides: int = 10,
                           cell_w_px: int = 160,
                           cell_h_px: int = 160) -> pathlib.Path:
    out = root / "listing_matrix.xlsx"
    with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
        df_sum = pd.DataFrame(summary_rows)
        if not df_sum.empty:
            cols = ["–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç", "–ê—Ä—Ç–∏–∫—É–ª", "–ë—Ä–µ–Ω–¥", "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "–°–ª–∞–π–¥—ã", "–ü–∞–ø–∫–∞"]
            df_sum = df_sum[[c for c in cols if c in df_sum.columns]]
        df_sum.to_excel(writer, sheet_name="–°–≤–æ–¥–∫–∞", index=False)

        wb = writer.book
        ws = wb.add_worksheet("–ú–∞—Ç—Ä–∏—Ü–∞")

        competitors = sorted([p for p in root.iterdir() if p.is_dir()])
        nm_ids = [c.name.split("_")[-1] for c in competitors]

        header_fmt = wb.add_format({"bold": True, "align": "center"})
        ws.write(0, 0, "")
        for col, nm in enumerate(nm_ids, start=1):
            ws.write(0, col, nm, header_fmt)

        row_labels_fmt = wb.add_format({"align": "center"})
        for r in range(1, limit_slides + 1):
            ws.write(r, 0, f"{r} —Å–ª–∞–π–¥", row_labels_fmt)

        col_width_chars = max(12, int(cell_w_px / 7))
        row_height_pts = max(24, int(cell_h_px / 1.33))
        ws.set_column(0, 0, 12)
        for c in range(1, len(nm_ids) + 1):
            ws.set_column(c, c, col_width_chars)
        for r in range(1, limit_slides + 1):
            ws.set_row(r, row_height_pts)

        x_offset = 5
        y_offset = 5

        for col, comp_dir in enumerate(competitors, start=1):
            imgs = sorted(list(comp_dir.glob("*.jpg")) + list(comp_dir.glob("*.webp")),
                          key=lambda p: (int(p.stem) if p.stem.isdigit() else 9999))
            for r_idx in range(limit_slides):
                if r_idx < len(imgs):
                    bio = _image_to_png_bytes(imgs[r_idx], cell_w_px, cell_h_px)
                    if bio:
                        ws.insert_image(r_idx + 1, col, imgs[r_idx].name,
                                        {"image_data": bio, "x_offset": x_offset, "y_offset": y_offset})
    return out

def save_collage(root: pathlib.Path, limit_slides: int = 10) -> pathlib.Path | None:
    competitors = sorted([p for p in root.iterdir() if p.is_dir()])
    if not competitors:
        return None
    grid = []
    max_rows = 0
    for c in competitors:
        imgs = sorted(list(c.glob("*.jpg")) + list(c.glob("*.webp")),
                      key=lambda p: (int(p.stem) if p.stem.isdigit() else 9999))
        imgs = imgs[:limit_slides]
        max_rows = max(max_rows, len(imgs))
        grid.append(imgs)
    if max_rows == 0:
        return None
    cols = len(grid)
    rows = max_rows
    cell_w, cell_h = THUMB
    pad = 10
    W = cols * cell_w + (cols + 1) * pad
    H = rows * cell_h + (rows + 1) * pad
    canvas = Image.new("RGB", (W, H), (245, 245, 245))
    for x, col_imgs in enumerate(grid):
        for y in range(rows):
            if y < len(col_imgs):
                try:
                    img = Image.open(col_imgs[y]).convert("RGB")
                    img.thumbnail(THUMB)
                    ox = pad + x * (cell_w + pad) + (cell_w - img.width)//2
                    oy = pad + y * (cell_h + pad) + (cell_h - img.height)//2
                    canvas.paste(img, (ox, oy))
                except Exception:
                    pass
    out = root / "matrix_preview.jpg"
    canvas.save(out, format="JPEG", quality=85)
    return out

def make_zip_bytes(root: pathlib.Path) -> bytes:
    mem = io.BytesIO()
    with zipfile.ZipFile(mem, mode="w", compression=zipfile.ZIP_DEFLATED) as z:
        for path in root.rglob("*"):
            if path.is_file():
                z.write(path, arcname=str(path.relative_to(root)))
    mem.seek(0)
    return mem.read()

# ================== –ò–ù–¢–ï–†–§–ï–ô–° ==================
with st.form("form_links"):
    urls_text = st.text_area("–°—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã WB (–ø–æ –æ–¥–Ω–æ–π –Ω–∞ —Å—Ç—Ä–æ–∫–µ)", height=160)
    session_name = st.text_input("–ò–º—è –Ω–∞–±–æ—Ä–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)", placeholder="–ê–Ω–∞–ª–∏–∑_—Ç–æ–≤–∞—Ä–æ–≤")
    detailed = st.checkbox("–î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å (–ø–æ —Ñ–æ—Ç–æ)", value=False)
    c1, c2 = st.columns(2)
    with c1:
        do_generate = st.form_submit_button("üöÄ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç")
    with c2:
        do_download_zip = st.form_submit_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤")

for key, default in [
    ("zip_bytes", None),
    ("zip_name", None),
    ("excel_bytes", None),
    ("excel_name", None),
    ("collage_bytes", None),
    ("collage_name", None),
]:
    if key not in st.session_state:
        st.session_state[key] = default

# ================== –û–°–ù–û–í–ù–û–ô –•–û–î ==================
if do_generate:
    links = parse_input_urls(urls_text)
    if not links:
        st.error("–î–æ–±–∞–≤—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —Å—Å—ã–ª–∫—É."); st.stop()

    root = new_unique_root(session_name)
    reqs = make_requests_session()

    overall = st.progress(0.0)
    overall_text = st.empty()

    ok_list, err_list = [], []
    total = len(links)

    # –î–ª—è fast-—Ä–µ–∂–∏–º–∞ ‚Äî —Å–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –ø–∞—á–∫–æ–π
    fast_items: list[tuple[int, int, pathlib.Path]] = []

    for idx, url in enumerate(links, start=1):
        overall_text.write(f"–¢–æ–≤–∞—Ä {idx}/{total}: {url}")

        nm_raw = extract_nm_id(url)
        if not nm_raw:
            err_list.append((url, "–ù–µ –Ω–∞–π–¥–µ–Ω –∞—Ä—Ç–∏–∫—É–ª (nm_id)"))
            overall.progress(idx/total); continue

        nm = int(nm_raw)
        try:
            prod = fetch_card_json(reqs, nm_raw)
        except Exception as e:
            err_list.append((url, f"API –æ—à–∏–±–∫–∞: {e}"))
            overall.progress(idx/total); continue

        title, brand, pics = parse_basics(prod)
        if pics <= 0:
            pics = DEFAULT_SLIDES

        subdir = root / f"{idx:03d}_{nm}"
        ensure_dir(subdir)
        (subdir / "meta.json").write_text(
            json.dumps({"url": url, "nm_id": nm, "title": title, "brand": brand,
                        "saved_at": datetime.now().isoformat()}, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )

        if detailed:
            exp = st.expander(f"üì¶ {idx}/{total} ‚Ä¢ nm={nm} ‚Ä¢ {title or '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'}", expanded=True)
            with exp:
                pbar = st.progress(0.0)
                line = st.empty()
                saved = download_product_images_detailed_sync(reqs, nm, pics, subdir, pbar, line)
            if saved > 0:
                ok_list.append((url, subdir.name, saved))
            else:
                err_list.append((url, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"))
            overall.progress(idx/total)
        else:
            # –≤ fast-—Ä–µ–∂–∏–º–µ –Ω–µ –∫–∞—á–∞–µ–º —Å—Ä–∞–∑—É ‚Äî —Å–¥–µ–ª–∞–µ–º –ø–∞—á–∫–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            fast_items.append((nm, pics, subdir))
            overall.progress(idx/total)

    # –µ—Å–ª–∏ –±—ã–ª fast-—Ä–µ–∂–∏–º ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –ø–∞—á–∫—É
    if fast_items:
        with st.spinner("‚ö° –ë—ã—Å—Ç—Ä–∞—è —Å–∫–∞—á–∫–∞ —Ñ–æ—Ç–æ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)‚Ä¶"):
            try:
                result_map = asyncio.run(run_fast_batch(fast_items))  # {nm: saved}
            except RuntimeError:
                # –µ—Å–ª–∏ Streamlit —É–∂–µ –∏–º–µ–µ—Ç —Ü–∏–∫–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—É—Å–∫
                loop = asyncio.new_event_loop()
                try:
                    asyncio.set_event_loop(loop)
                    result_map = loop.run_until_complete(run_fast_batch(fast_items))
                finally:
                    loop.close()

        # —Å–æ–±–∏—Ä–∞–µ–º –æ–∫/–æ—à–∏–±–∫–∏ –ø–æ fast-—Å–ø–∏—Å–∫—É
        for (nm, pics, subdir) in fast_items:
            saved = int(result_map.get(nm, 0))
            url = None
            meta = subdir / "meta.json"
            if meta.exists():
                try:
                    m = json.loads(meta.read_text(encoding="utf-8"))
                    url = m.get("url")
                except Exception:
                    pass
            if saved > 0:
                ok_list.append((url or f"nm={nm}", subdir.name, saved))
            else:
                err_list.append((url or f"nm={nm}", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"))

    # ---- –°–≤–æ–¥–∫–∞ / Excel / –ö–æ–ª–ª–∞–∂ ----
    competitors = sorted([p for p in root.iterdir() if p.is_dir()])
    summary_rows = []
    for sub in competitors:
        nm = sub.name.split("_")[-1]
        imgs = sorted(list(sub.glob("*.jpg")) + list(sub.glob("*.webp")),
                      key=lambda p: (int(p.stem) if p.stem.isdigit() else 9999))
        title = brand = None
        meta = sub / "meta.json"
        if meta.exists():
            try:
                m = json.loads(meta.read_text(encoding="utf-8"))
                title, brand = m.get("title"), m.get("brand")
            except Exception:
                pass
        summary_rows.append({
            "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç": sub.name.split("_")[0],
            "–ê—Ä—Ç–∏–∫—É–ª": nm,
            "–ë—Ä–µ–Ω–¥": brand,
            "–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": title,
            "–°–ª–∞–π–¥—ã": len(imgs),
            "–ü–∞–ø–∫–∞": sub.name,
        })

    max_slides = detect_max_slides(root)
    xlsx_path = save_excel_with_images(root, summary_rows, limit_slides=max_slides,
                                       cell_w_px=CELL_PX[0], cell_h_px=CELL_PX[1])
    collage_path = save_collage(root, min(max_slides, 10))

    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –≤ –ø–∞–º—è—Ç—å
    with open(xlsx_path, "rb") as f:
        excel_bytes = f.read()
    excel_name = xlsx_path.name

    collage_bytes = None
    collage_name = None
    if collage_path and collage_path.exists():
        with open(collage_path, "rb") as f:
            collage_bytes = f.read()
        collage_name = collage_path.name

    # ZIP
    zip_bytes = make_zip_bytes(root)
    zip_name = f"{root.name}.zip"

    # –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ
    try:
        shutil.rmtree(root, ignore_errors=True)
    except Exception:
        pass

    # –í —Å–µ—Å—Å–∏—é
    st.session_state["zip_bytes"] = zip_bytes
    st.session_state["zip_name"] = zip_name
    st.session_state["excel_bytes"] = excel_bytes
    st.session_state["excel_name"] = excel_name
    st.session_state["collage_bytes"] = collage_bytes
    st.session_state["collage_name"] = collage_name

    st.success("–ì–æ—Ç–æ–≤–æ! –ü–∞–∫–µ—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω. –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ —É–¥–∞–ª–µ–Ω–∞.")
    st.write(f"üìä Excel: {excel_name}")
    if collage_name:
        st.write(f"üñº –ö–æ–ª–ª–∞–∂: {collage_name}")

    if ok_list:
        st.subheader("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        for url, folder, cnt in ok_list:
            st.write(f"- {folder} ‚Äî {cnt} —Ñ–æ—Ç–æ ‚Äî {url}")
    if err_list:
        st.subheader("‚ö†Ô∏è –û—à–∏–±–∫–∏")
        for url, msg in err_list:
            st.write(f"- {url}: {msg}")

    # –ö–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    if st.session_state["excel_bytes"]:
        st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ç–æ–ª—å–∫–æ Excel",
                           data=st.session_state["excel_bytes"],
                           file_name=st.session_state["excel_name"],
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
    if st.session_state["collage_bytes"]:
        st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–ª–ª–∞–∂ (JPG)",
                           data=st.session_state["collage_bytes"],
                           file_name=st.session_state["collage_name"],
                           mime="image/jpeg")
    st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤ (–≤—Å—ë –≤–º–µ—Å—Ç–µ)",
                       data=st.session_state["zip_bytes"],
                       file_name=st.session_state["zip_name"],
                       mime="application/zip")

# –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ ZIP
if do_download_zip:
    if not st.session_state["zip_bytes"]:
        st.error("–ê—Ä—Ö–∏–≤ –µ—â—ë –Ω–µ –≥–æ—Ç–æ–≤. –°–Ω–∞—á–∞–ª–∞ –Ω–∞–∂–º–∏ ¬´–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞–∫–µ—Ç¬ª.")
    else:
        st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –∞—Ä—Ö–∏–≤ (–≤—Å—ë –≤–º–µ—Å—Ç–µ)",
                           data=st.session_state["zip_bytes"],
                           file_name=st.session_state["zip_name"],
                           mime="application/zip")
